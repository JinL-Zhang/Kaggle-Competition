{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTs11WubmRhV"
   },
   "outputs": [],
   "source": [
    "# download data (-q is the quiet mode)\n",
    "! wget -q https://github.com/CISC-372/Notebook/releases/download/a1/test.csv -O test.csv\n",
    "! wget -q https://github.com/CISC-372/Notebook/releases/download/a1/train.csv -O train.csv\n",
    "# you can tuning the model (search for the best hyper-parameters setting) automatically if we have a narrow range of hyper-parameter to be searched for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "jrsga6qkouO1",
    "outputId": "0afd0d67-f590-4ed3-e5d8-77168e3a44bb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# The dataset contain rental house information, where each data sample (data row) represents a rental post\n",
    "  \n",
    "# we can do data pre-processing with pandas or build it into the pieline for hyper-parameter tuning \n",
    "Xy_train = pd.read_csv('train.csv', engine='python')\n",
    "X_train = Xy_train.drop(columns=['price_rating'])\n",
    "y_train = Xy_train[['price_rating']]\n",
    "\n",
    "print('training', len(X_train))\n",
    "#Xy_train.price_rating.hist()\n",
    "\n",
    "X_test = pd.read_csv('test.csv', engine='python')\n",
    "testing_ids = X_test.Id\n",
    "print('testing', len(X_test))\n",
    "\n",
    "# Note: The pre-processing steps are split, below are some basic pre-processing done through Pandas dataframe \n",
    "\n",
    "# In the original training and testing datasets, the attribute 'deposit' and 'extra_people' has '$' as prefix, ',' as infix , and 'host_response_rate' has '%' as suffix\n",
    "# so we remove the '$', ',' , '%' and convert those string integers into float values \n",
    "X_train['security_deposit'] = Xy_train['security_deposit'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "X_train['extra_people'] = Xy_train['extra_people'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "X_train['host_response_rate'] = Xy_train['host_response_rate'].replace({'%': ''}, regex=True).astype(float) / 100 # divided by 100 to transform from '%' representation to ordinary numeric representation\n",
    "\n",
    "# The attribute 'deposit', 'extra_people', and 'host_response_rate' values in X_test must also be converted into float type values\n",
    "# So that our model can be applied to the testing set\n",
    "X_test['security_deposit'] = X_test['security_deposit'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "X_test['extra_people'] = X_test['extra_people'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "X_test['host_response_rate'] = X_test['host_response_rate'].replace({'%': ''}, regex=True).astype(float) / 100\n",
    "\n",
    "# For time-series attribute 'host_since' and 'last_scraped', we can combine them to create a new numeric feature to be added into the feature space\n",
    "training_days_active = pd.to_datetime(Xy_train['last_scraped']) - pd.to_datetime(Xy_train['host_since']) # This is a pandas Seres about the number of days that a host has been on the platform\n",
    "testing_days_active = pd.to_datetime(X_test['last_scraped']) - pd.to_datetime(X_test['host_since'])\n",
    "# Create a new numeric feature, named 'host_days_active', based on the 'host_since' and 'last_scraped'\n",
    "X_train['host_days_active'] = training_days_active.astype('timedelta64[D]')\n",
    "X_test['host_days_active'] = testing_days_active.astype('timedelta64[D]')\n",
    "\n",
    "# For time-series attribute 'first_review' and 'last_review', we can combine them to create a new numeric feature to be added into the feature space\n",
    "training_review_active = pd.to_datetime(Xy_train['last_review']) - pd.to_datetime(Xy_train['first_review']) # This is a pandas Seres about the number of days that reviews are being written for the rental listings\n",
    "testing_review_active = pd.to_datetime(X_test['last_review']) - pd.to_datetime(X_test['first_review'])\n",
    "# Create a new numeric feature, named 'review_active', based on the 'first_review' and 'last_review'\n",
    "X_train['review_active'] = training_review_active.astype('timedelta64[D]')\n",
    "X_test['review_active'] = testing_review_active.astype('timedelta64[D]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual tuning approach: \n",
    "1. Split the training set into 2 subsets (as the training set have the known target attribute values), one subset for training/building the model, and the another one for evaluating the model as the 1st validation set\n",
    "2. Then, based on the model's performance on the 1st validation set, we can do hyper-parameter tuning to adjust the model\n",
    "3. Once a new model with good hyper-parameter settings is gained, we test the model on the entire training set (put the 2 subsets back togeter) namely adjusting the model again based on its performance on Xy_train (but we should not change the hyper-parameter settings as we have already optimized its)\n",
    "4. Then, we can apply our model to the 2nd validation set (testing set in public leaderboard)\n",
    "\n",
    "# Semi-auto tuning approach: (using hold-out method)\n",
    " 1. split Xy_train into training set and 1st validation set\n",
    " 2. pick a range of hyper-parameters (ex: regularization, learning rate,etc)\n",
    " 2. training set -> build all the models based on the hyper-parameter range we choose\n",
    " 3. validation set -> evaluate all the models we gained in step 3 -> adjust the hyper-parameter ranges and change the model (go back step 2 if needed)\n",
    " 4. train a new model using the chosen hyper-parameters on Xy_train, and evaluate on X_test\n",
    "\n",
    "# Semi-auto tuning approach: (using cross-validation method)\n",
    " 1. pick a range of hyper-parameters (ex: pre-processing, data selections, regularization, learning rate,etc)\n",
    " 2. train/evaluate models using CV on the training set, Xy_train\n",
    " 3. Based on the results of CV, we adjust the hyper-parameter ranges or change the model (go back step 2 if needed)\n",
    " 4. train a new model using the chosen/ideal hyper-parameters on Xy_train, and evaluate on X_test\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "3vFDJ3lNmE6P",
    "outputId": "ea3e89a7-c3e7-45cb-d567-3679a8209a6a"
   },
   "outputs": [],
   "source": [
    "# model training and tuning\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, OneHotEncoder,OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "np.random.seed(0) # set the randomizer to default, so we can get the same sequence of data rows in each fold of CV every time we re-run the code\n",
    "# so we can better analyze the performance of the model as the impact brought by the randomization in CV is removed\n",
    "\n",
    "# select needed data attributes for classification purpose (as the target attribute is categorical data [0,1,2])\n",
    "# and we can select different pre-processing techniques for different types of attributes (numeric vs categorical)\n",
    "# so we need to treat different types of attributes separately\n",
    "\n",
    "# increase model performance by selecting more attributes \n",
    "numeric_features = ['bedrooms', 'review_scores_location','host_total_listings_count', 'availability_60','accommodates', 'beds', 'bathrooms',\n",
    "                    'availability_90','guests_included', 'minimum_nights','maximum_nights', 'review_scores_rating',\n",
    "                    'reviews_per_month','availability_365','availability_30','review_scores_accuracy','review_scores_value', \n",
    "                    'review_scores_cleanliness','security_deposit','extra_people','review_scores_communication', \n",
    "                    'review_scores_checkin', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm',\n",
    "                   'number_of_reviews_ltm', 'host_response_rate',\n",
    "                   'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count', 'number_of_reviews','calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms',\n",
    "                    'minimum_minimum_nights','maximum_minimum_nights','minimum_maximum_nights','maximum_maximum_nights', \n",
    "                    # 'host_days_active','review_active', Note: these two attributes are commented out as adding them into the training set reduces the model's performance\n",
    "                   ] # select needed numeric features from the training dataset\n",
    "\n",
    "# Define a transformer/pre-processor for numeric attributes\n",
    "# pipeline() means we can have different steps in pre-processing, and in each step, we can transform the features (we can have as many steps as we want)\n",
    "# Also, we need to give a name to each step in pipeline as we need to specify the range of hyper-parameters by specifying which step we want to adjust later\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('iterative_imputer', IterativeImputer(max_iter=10, random_state=0)),\n",
    "    #    ('imputer', SimpleImputer(strategy='median')), # the first step is called 'imputer', which replaces the missing value with the median attribute value, but it did not yield a better performance than 'iterative_imputer'\n",
    "    ('scaler', StandardScaler())]) # the second step is called 'scaler', which transfers each entry in numeric data columns to have zero mean and unit variance with respect to the column it is in\n",
    "\n",
    "# select categorical features\n",
    "categorical_features = [\n",
    "  'property_type', 'is_business_travel_ready', 'room_type', 'bed_type', 'is_location_exact','host_identity_verified',\n",
    "  'host_response_time','require_guest_profile_picture','require_guest_phone_verification','has_availability', \n",
    "  'cancellation_policy','host_is_superhost','instant_bookable', \n",
    "    'calendar_updated', 'requires_license', \n",
    "    \n",
    "] \n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # replace all the missing values with the constant string 'missing' \n",
    "    # note the step names in different pipelines can be the same\n",
    "#    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)) # try ordinal encoder\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# ex of onehot encoder: if 'property_type' have value = [apt,house,room] \n",
    "# After encoding, we will get three different features with each having the value [0,1]:\n",
    "# property_type_apt = [0,1], so 1 stands for the observation having 'apt' at property_type, 0 for other property_type\n",
    "# property_type_house = [0,1]\n",
    "# property_type_room = [0,1]\n",
    "\n",
    "# ColumnTransformer transforms each of the selected data column (it will transform all the selected attributes in training and testing sets)\n",
    "preprocessor = ColumnTransformer(  # apply categorical_transformer to categorical_features, and apply numeric_transformer to numeric_features\n",
    "    transformers=[    # define list of transformer we want  \n",
    "        ('num', numeric_transformer, numeric_features), # give a name 'num' to the numeric_transformer to pre-process the numeric_features\n",
    "        ('cat', categorical_transformer, categorical_features)]) # give a name 'cat' to the categorical_transformer to pre-process the categorical_features\n",
    "# note that each transformer requires an input list of features, and\n",
    "# we have 2 different transformers because we have different pre-processing techniques for different types of attributes (categorical vs numeric)\n",
    "# and you can have as many transformers as you want in the ColumnTransformer()\n",
    "\n",
    "# define the whole pipeline of building/training the classifier/model by combining the pre-processor to the model building process\n",
    "regr = Pipeline(steps=[('preprocessor', preprocessor), # the first step is do the pre-processing with using the pre-processor we defined above, and the pre-processor will process different features based on which type group it is in with using the corresponding transformer\n",
    "                      #('standardscaler', StandardScaler(copy=False, with_mean=False)), # standardization on the training set, not needed as they are in the preprocessor already \n",
    "                       ('normalizer', Normalizer()), # the second step is to integrate regularization into the model building process to avoid overfitting \n",
    "                       ('classifier', LogisticRegression(random_state=123, multi_class='multinomial'))]) # The third step is choosing Logistic Regression model for the classification problem                    \n",
    "\n",
    "\n",
    "# Feature selection: select the needed non-target attributes from the updated training and testing sets\n",
    "X_train = X_train[[*numeric_features, *categorical_features]] # [*numeric_features, *categorical_features] merges two independent lists into one, instead of merging into a list of lists\n",
    "X_test = X_test[[*numeric_features, *categorical_features]]\n",
    "\n",
    "# `__` denotes attribute of the previous ONE term/name\n",
    "# (e.g. regressor__n_estimators means the `n_estimators` parameter for `classifier`\n",
    "#  which is our xgb)\n",
    "\n",
    "# try RandomSearchCV\n",
    "distributions = { # set the range of (hyper-)parameters we want to search,\n",
    "    'preprocessor__num__iterative_imputer__max_iter': range(10,20), # The default value of 'max_iter' is 10, and there is no need to extend further the range(10,20) as the best setting for the 'max_iter' is always below 18\n",
    "#    'preprocessor__num__imputer__strategy': ['mean','median','most_frequent'], # search the optimal hyper-parameter setting for the pre-processor 'imputer', but it did not yield a better model performance than 'iterative_imputer'\n",
    "    'classifier__max_iter': range(200,400), # defautl value is 100, we set the range to be (200,400) as training longer can avoid underfitting\n",
    "    'classifier__solver': ['newton-cg','lbfgs', 'sag', 'saga' ], # search which algorithm is the best to be used in the optimization problem\n",
    "    'classifier__tol': [1e-4,1e-5,], # note that '1e-5' tends to not converge when the 'max_iter' is reached\n",
    "    'classifier__class_weight': ['balanced',None], # the default value is None, but it turns out that 'balanced' is the best setting for this parameter in most of situations  \n",
    "    'normalizer__norm': ['l1','l2','max'] # 'max' turns out to be the best settting for the regularization strategy\n",
    "    \n",
    "    #'classifier__penalty': ['l2','none','elasticnet'] #see if regularization needed, the default value is 'l2', and it turns out tuning this parameter is redundant as we already have a normalizer() in the search space    \n",
    "} \n",
    "\n",
    "# Adjustment log (previous records/logs are lost due to some data storage problem):\n",
    "# 9th tuning: remove Normalizer() from regr to see if it is redundant\n",
    "# result: Performance decreases dramatically (need to add regularization back), and 'balanced' should be the best parameter setting for 'class_weight'\n",
    "\n",
    "# 10th tuning: add Normalizer back, extend 'max_iter' to range(200,400)\n",
    "# result: Performance did not improve, may need to change the model architecture or remove some attributes from the training set\n",
    "\n",
    "# fit the model on the full training dataset with using CV, namely the step 4 in the Semi-auto tuning apporoach\n",
    "random_search_log = RandomizedSearchCV(regr, distributions, n_iter=40,random_state=0,scoring = 'f1_micro', # f1 with 'micro-averaging' in a multiclass setting is chosen, and will return the total ratio of tp/(tp + fp)   \n",
    "                                       n_jobs = -1, cv=5, verbose=1 )\n",
    "# n_jobs = -1 means using all the CPU processors, random_state=0 to ensure we get the same result/performance each time we run this cell of code\n",
    "random_search_log.fit(X_train, y_train)\n",
    "print('best score {}'.format(random_search_log.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocessor__num__iterative_imputer__max_iter': 18,\n",
       " 'normalizer__norm': 'l2',\n",
       " 'classifier__tol': 1e-05,\n",
       " 'classifier__solver': 'sag',\n",
       " 'classifier__max_iter': 239,\n",
       " 'classifier__class_weight': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get feedbacks, see what are the best (hyper-)parameter settings in the search space we specified above\n",
    "random_search_log.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feedbacks, determine the performance of the model on the training set, and the evaluation metric is set to be 'accuracy' (as we want to check the model's accuracy first)\n",
    "from sklearn.model_selection import cross_validate\n",
    "scoring = ['accuracy'] \n",
    "scores = cross_validate(random_search_log.best_estimator_, X_train, y_train, scoring=scoring, cv=5, n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([9.49279404, 6.90370774, 9.02439189, 9.41852403, 3.88465691]),\n",
       " 'score_time': array([0.03730392, 0.05217409, 0.06174397, 0.03809118, 0.03446078]),\n",
       " 'test_f1_micro': array([0.69679109, 0.71952818, 0.72083879, 0.7129751 , 0.70642202])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores # print the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PF6WrzdKmJ97"
   },
   "outputs": [],
   "source": [
    "# Prediction & generating the submission file\n",
    "y_pred = random_search_log.predict(X_test) # generate the predictions on the testing set with using the model/classifier we trained/tuned above\n",
    "pd.DataFrame( #construct a dataframe with the 'Id' value of the observations and the predictions of testing 'price_rating' values we gained above, then export the dataframe as a CSV file that can be submitted to the Kaggle leaderboard\n",
    "    {'Id': testing_ids, 'price_rating':y_pred}).to_csv('sample_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define another pipeline of building/training a XGB classifier/model with using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 25.1min\n",
      "[Parallel(n_jobs=-1)]: Done 175 out of 175 | elapsed: 33.6min finished\n",
      "/opt/anaconda3/envs/test372/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/test372/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/envs/test372/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:12:10] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.7307058358030764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "regr = Pipeline(steps=[('preprocessor', preprocessor), # the first step is do the pre-processing with using the pre-processor we defined above, and the pre-processor will process different features based on which type group it is in with using the corresponding transformer\n",
    "#                     ('normalizer', Normalizer()), # the second step is to integrate regularization into the model building process to avoid overfitting \n",
    "                       ('classifier', XGBClassifier( # the third step is choosing the model architecture we will use\n",
    "                         seed=1, ))]) # set the number of classes, num_class=3\n",
    "param_random = { # the range of hyper-parameters we want to search\n",
    "    'preprocessor__num__iterative_imputer__max_iter': range(10,20),# The default value of 'max_iter' is 10, \n",
    " #   'preprocessor__num__imputer__strategy': ['mean'],#'median','most_frequent'], # search the optimal hyper-parameter setting for the pre-processor 'imputer', but it failed to yield a better model performance than 'iterative_imputer'\n",
    " #   'preprocessor__cat__onehot__drop': ['first','if_binary',None], #Tuning the categorical pre-processor, the default value is 'None', and it turns out that 'None' is the one yielding the best model performance in most of the situations\n",
    "    'classifier__objective': ['multi:softmax', 'multi:softprob'],#'rank:pairwise','rank:map' ], tuning the objective function, but it turns out 'multi:softmax' is the best parameter setting in most of the cases\n",
    "    #'classifier__eval_metric': ['merror','map','mlogloss','aucpr'], # Tuning this hyperparameter does not affect the performance of the model at all\n",
    "    'classifier__max_depth': range(6,30), # The default value is 6, so we make it more flexible by extending the max_depth from the default value 6 to 30\n",
    "    'classifier__n_estimators': range(200,400), # The default value is 100, so we make it more flexible by extending the number of estimators from the default value 100 to the range(200,400)\n",
    "    'classifier__colsample_bynode': np.arange(0.0, 1.1, 0.1), # the ranges of colsample_bynode, colsample_bylevel, and colsample_bytree are all (0,1], so we use np.arange() rather than range()\n",
    "    'classifier__colsample_bytree': np.arange(0.0, 1.1, 0.1), # np.arange(0.0, 1.1, 0.1) gives a array of float values from 0.0 to 1.0 with incrementing each element by 0.1 \n",
    "    'classifier__colsample_bylevel': np.arange(0.0, 1.1, 0.1), \n",
    "    # try np.arange(0.0,1.0,0.05) to see if it give better performance than (0.0, 1.1, 0.1), but it turns out that (0.0,1.0,0.05) does not give a better performance  \n",
    "#    'classifier__colsample_bynode': np.arange(0.0,1.0,0.05), # the ranges of colsample_bynode, colsample_bylevel, and colsample_bytree are all (0,1], so we use np.arange() rather than range()\n",
    "#    'classifier__colsample_bytree': np.arange(0.0,1.0,0.05), # np.arange(0.0,1.05,0.05) gives a array of float values from 0.0 to 1.0 with incrementing each element by 0.05 \n",
    "#    'classifier__colsample_bylevel': np.arange(0.0,1.0,0.05),\n",
    "#    'classifier__booster':['gbtree', 'gblinear', 'dart'], # check which booster performs better, and it turns out that the default 'gbtree' is always better than the other two boosters\n",
    "    'classifier__min_child_weight': range(0,10), # the larger the min_child_weight(default value is 1) and max_delta_step(default value is 0) values are, the more conservative the algorithm will be\n",
    "#    'classifier__max_delta_step': range(0,10), # However, tuning the 'max_delta_step' tends to deteriorate the model's performance\n",
    "    'classifier__eta': np.arange(0.01,0.2,0.01), # adjust the learning rate, but the performance of the model is worse when we tuning the learning rate in a range that is above the default value 0.3, such as (0.4,1.0,0.1)\n",
    "#    'classifier__scale_pos_weight': range(1,10), # we adjust the balance of positive and negative weights, the default value is 1, but tuning this parameter is not recommended by the system, and it did not improve the model's performance too\n",
    "    'classifier__gamma': range(0,10), # default value of gamma is 0, and the larger gamma is, the more conservative the algorithm will be\n",
    "#    'classifier__tree_method': ['auto','hist'], # the default value is 'auto', and it turns out that the default value will yield the best performance of the model in most of the situations \n",
    "#    'normalizer__norm': ['max'], # ,'l1','l2'], 'max' turns out to be the best settting for the regularization strategy, however, it later turns out that tuning the model's regularization parameter will be better than tuning this Normalizer in the model's pipeline\n",
    "    'classifier__lambda': range(1,6), # L2 regularization term on weights, the default value is 1, and there is no need to extend further the range(1,6) as the best setting for the 'lambda' is always below 3\n",
    "    'classifier__alpha': range(0,6), # L1 regularization term on weights, the default value is 0, and there is no need to extend further the range(0,6) as the best setting for the 'alpha' is always below 3\n",
    "}\n",
    "# Adjustment log (previous records/logs are lost due to some data storage problem): \n",
    "# 24th tuning: remove 'l1','l2' from normalizer__norm, remove classifier__colsample_bylevel, replace 'exact' with 'hist' in 'tree_method' to see if performance increase\n",
    "# result: performance (accuracy score) did not improve (may because we did not tune 'colsample_bylevel'), and 'auto' is still the best setting for 'tree_method' parameter\n",
    "\n",
    "# 25th tuning: put back classifier__colsample_bylevel for tuning, change 'eta' tuning range from (0.1,0.3,0.1) to (0.01,0.2,0.01)\n",
    "# result: performance in both training set and the validation set improve to 73.09%\n",
    "\n",
    "# 26th tuning: Same parameter setting with 25th tuning, but add 'host_days_active' and 'review_active' attributes into the feature space\n",
    "# result: performance decreases\n",
    "\n",
    "# 27th tuning: change 'eta' from (0.01,0.2,0.01) back to (0.1,0.3,0.1)\n",
    "# result: performance did not improve (may need to remove the added 'host_days_active' and 'review_active' attributes)\n",
    "\n",
    "# 28th: remove Normalizer() parameter and 'host_days_active' and 'review_active' attributes\n",
    "# result: Performances in both training set and the validation set improve to 73.5%\n",
    "\n",
    "# 29th: Tuning the model's regularization parameter 'lambda' and 'alpha'\n",
    "# result: Performance on both training set and the validation set improve to 73.6% (The end of Kaggle competition)\n",
    "\n",
    "# Originally, I used GridSearchCV() for XGBooster model training, but it takes 50 mins to train and can only search a few (hyper-)parameters at one run, so I shifted to randomSearchCV\n",
    "random_search = RandomizedSearchCV( # pass the model pipeline and the ranges of (hyper-)parameters we want to search as arguments to RandomizedSearchCV()\n",
    "    regr, param_random, cv=5, verbose=1, n_jobs=-1,  # cv=5 means we have 5 folds for the CV, n_jobs = -1 means using all CPU processors\n",
    "    n_iter=35,random_state=1,  # 'n_iter'=35 means that 35 parameter settings are randomly sampled, and so we will have 35 models that will go through the 5-fold cross-validation\n",
    "    scoring='f1_micro') \n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print('best score {}'.format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocessor__num__iterative_imputer__max_iter': 14,\n",
       " 'classifier__objective': 'multi:softmax',\n",
       " 'classifier__n_estimators': 307,\n",
       " 'classifier__min_child_weight': 3,\n",
       " 'classifier__max_depth': 8,\n",
       " 'classifier__lambda': 1,\n",
       " 'classifier__gamma': 4,\n",
       " 'classifier__eta': 0.19,\n",
       " 'classifier__colsample_bytree': 0.7000000000000001,\n",
       " 'classifier__colsample_bynode': 1.0,\n",
       " 'classifier__colsample_bylevel': 1.0,\n",
       " 'classifier__alpha': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get feedbacks, see what are the best (hyper-)parameter settings in the search space we specified above\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([116.01877117,  85.75852799, 116.25719094, 116.19856882,\n",
       "         55.20052505]),\n",
       " 'score_time': array([0.08368492, 0.16831899, 0.08954906, 0.08655715, 0.04163504]),\n",
       " 'test_accuracy': array([0.71447282, 0.73328965, 0.73984273, 0.73525557, 0.73066841])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get feedbacks, determine the performance of the model on the training set, and the evaluation metric is set to be 'accuracy' \n",
    "scoring = ['accuracy']  \n",
    "scores = cross_validate(random_search.best_estimator_, X_train, y_train, scoring=scoring, cv=5, n_jobs=-1) \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction & generating the submission file\n",
    "y_pred = random_search.best_estimator_.predict(X_test) # generate the predictions on the testing set with using the model/classifier we trained/tuned above\n",
    "pd.DataFrame( #construct a dataframe with the 'Id' value of the observations and the predictions of testing 'price_rating' values we gained above, then export the dataframe as a CSV file that can be submitted to the Kaggle leaderboard\n",
    "    {'Id': testing_ids, 'price_rating':y_pred}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Define another pipeline of building/training a XGB classifier/model with using GridSearchCV for comparison purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/test372/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  category=UserWarning\n",
      "/opt/anaconda3/envs/test372/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/test372/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:09:13] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:541: \n",
      "Parameters: { n_estimator } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:09:13] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr = Pipeline(steps=[('preprocessor', preprocessor), # the first step is do the pre-processing with using the pre-processor we defined above, and the pre-processor will process different features based on which type group it is in with using the corresponding transformer\n",
    "                     ('normalizer', Normalizer()), # the second step is to integrate regularization into the model building process to avoid overfitting \n",
    "                       ('classifier', XGBClassifier( # the third step is choosing the model architecture we will use\n",
    "                         seed=1, num_class=3 ))]) # set the number of classes, num_class=3\n",
    "grid_para = {'preprocessor__num__iterative_imputer__max_iter':[15],\n",
    "             'classifier__objective': ['multi:softmax', ],#'multi:softprob'],\n",
    "             'classifier__max_depth': [6,20],\n",
    "             'classifier__n_estimators': [200,300,],\n",
    "             'classifier__colsample_bynode': [0.4,0.7,0.9], # the ranges of colsample_bynode, colsample_bylevel, and colsample_bytree are all (0,1], \n",
    "            'classifier__colsample_bytree': [0.4,0.7,0.9], \n",
    "#            'classifier__colsample_bylevel': [0.4,0.7,0.9], these parameters are commented out as tuning them will increase the training time significantly\n",
    "#             'classifier__min_child_weight': [1,3,7],\n",
    "#             'classifier__max_delta_step': [1,3,7],\n",
    "#             'classifier__eta': [0.1,0.3,0.5]\n",
    "            }\n",
    "grid_search = GridSearchCV( \n",
    "    regr, grid_para, cv=5, verbose=3, n_jobs=-1, \n",
    "    scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "\n",
    "# The disadvantage of gridSearch is apparent, \n",
    "# the number of (hyper-)parameters and the numbers of the optional values for the parameters we are tuning are much less than the parameters we are tuning for the RandomSearchCV at one run \n",
    "# And if we want to tuning more parameters, then the total number of fits will be much higher than that of fits required by RandomSearchCV with n_iter set reasonably\n",
    "# Thus, the time taken to complete the GridSearchCV is much longer than RandomSearchCV if we want to tuning an adequate number of (hyper-)parameters\n",
    "# and the performances of the model gained from both GridSearchCV and RandomSearchCV tend to be similar if sufficient amount of training time are given to both of the methods,\n",
    "# while the performance gained from RandomSearchCV is likely to be better than the one gained from GridSearchCV if the training time is limited\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([29.28269911, 30.96060705, 30.16808128, 30.30623984,  9.86873031]),\n",
       " 'score_time': array([0.17380285, 0.06898999, 0.11868501, 0.1181221 , 0.06486988]),\n",
       " 'test_accuracy': array([0.69548134, 0.72018349, 0.73263434, 0.72804718, 0.72083879])}"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring = ['accuracy'] # you can include as many scores as you want\n",
    "scores = cross_validate(grid_search.best_estimator_, X_train, y_train, scoring=scoring, cv=5, n_jobs=-1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define another pipeline of building/training a different classifier/model with using Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "regr = Pipeline(steps=[('preprocessor', preprocessor), # the first step is do the pre-processing with using the pre-processor we defined above, and the pre-processor will process different features based on which type group it is in with using the corresponding transformer\n",
    "                     ('normalizer', Normalizer()), # the second step is to integrate regularization into the model building process to avoid overfitting \n",
    "                       ('classifier', SVC( # the third step is choosing the search space (which model we will use)\n",
    "                         max_iter=10000 ))])\n",
    "param_grid = { # the range of hyper-parameters we want to search\n",
    "   'preprocessor__num__iterative_imputer__max_iter': Integer (5,20),\n",
    "    # 'preprocessor__num__imputer__strategy': ['mean','median','most_frequent'], # search the optimal hyper-parameter setting for the pre-processor\n",
    "    'classifier__kernel': Categorical(['linear','poly', 'rbf', 'sigmoid','precomputed']), \n",
    "    'classifier__gamma': Categorical(['auto','scale']),\n",
    "    # note that for Integer and Real, we only need to supply the lower and upper bounds (inclusive), and random values will be sampled uniformly accoridng to the range we set\n",
    "    'classifier__degree': Integer(1,10), # set degree for poly kernel use only\n",
    "    'classifier__coef0': Integer(0,5), #  only used in ‘poly’ and ‘sigmoid’\n",
    "    'classifier__tol': Real(1e-4,1e-3),\n",
    "    'classifier__decision_function_shape': Categorical(['ovr','ovo']), # default value is 'ovr',and 'ovo' cannot be used when is 'break_ties=True'\n",
    "#    'classifier__break_ties': Categorical([True, False]), # This parameter can only be True when decision_function_shape='ovr', so the default value is False\n",
    "    'classifier__cache_size': Integer(200,300), # Specify the size of the kernel cache size (MB), the default value is 200\n",
    "    'normalizer__norm': Categorical(['l1','l2','max']),\n",
    "    }\n",
    "\n",
    "# adjustment log\n",
    "# 1st Tuning Result: 72% in training set\n",
    "\n",
    "# 2nd Tuning: Added parameter 'decision_function_shape', 'break_ties', 'cache_size' for tuning\n",
    "# result: performance did not improve, 'False' is the best parameter setting for 'break_ties'\n",
    "\n",
    "# 3rd Tuning: Remove 'break_ties',\n",
    "# result: performance did not improve, may need to change the model architecture or remove/add some attributes from/to the training set, 'ovr' should be the best setting for 'decision_function_shape'\n",
    "\n",
    "bayes_search = BayesSearchCV( # putting the model pipeline, the range of hyper-parameters we want to search, into the BayesSearchCV\n",
    "    regr, param_grid, n_iter=50, n_points=10, \n",
    "    cv=5, random_state=0 ,verbose=1, n_jobs=-1, iid=True, # cv=5 means we have 5 folds for the CV, and n_jobs = 2 means number of CPU we want to use\n",
    "    scoring = 'f1_micro')\n",
    "\n",
    "bayes_search.fit(X_train, y_train)\n",
    "print('best score {}'.format(bayes_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('classifier__cache_size', 262),\n",
       "             ('classifier__coef0', 1),\n",
       "             ('classifier__decision_function_shape', 'ovr'),\n",
       "             ('classifier__degree', 10),\n",
       "             ('classifier__gamma', 'scale'),\n",
       "             ('classifier__kernel', 'rbf'),\n",
       "             ('classifier__tol', 0.00010000242082487538),\n",
       "             ('normalizer__norm', 'l1'),\n",
       "             ('preprocessor__num__iterative_imputer__max_iter', 6)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([17.98857689, 15.753829  , 17.69740081, 17.84777069,  9.77823997]),\n",
       " 'score_time': array([2.3238101 , 1.81304884, 2.43443131, 2.42977118, 1.26624489]),\n",
       " 'test_accuracy': array([0.69941061, 0.7293578 , 0.72346003, 0.72018349, 0.71428571])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring = ['accuracy'] \n",
    "scores = cross_validate(bayes_search.best_estimator_, X_train, y_train, scoring=scoring, cv=5, n_jobs=-1) \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction & generating the submission file\n",
    "y_pred = bayes_search.predict(X_test) # generate the predictions on the testing set with using the model/classifier we trained/tuned above\n",
    "pd.DataFrame( #construct a dataframe with the 'Id' value of the observations and the predictions of testing 'price_rating' values we gained above, then export the dataframe as a CSV file that can be submitted to the Kaggle leaderboard\n",
    "    {'Id': testing_ids, 'price_rating':y_pred}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization is slightly better than Randomized Search in this problem, because Bayesian Search spend about 21 mins to finish the (hyper-)parameters optimization, while Randomized Search spend about 24 mins (3 mins more than Bayesian Search), and the score gained by Bayesian Search is similar to that gained by Randomized Search (both scores are around '72%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 175 out of 175 | elapsed: 13.2min finished\n",
      "/opt/anaconda3/envs/test372/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7172082077004484\n"
     ]
    }
   ],
   "source": [
    "# SVM with RandomizedSearchCV for comparison purpose\n",
    "regr = Pipeline(steps=[('preprocessor', preprocessor), # the first step is do the pre-processing with using the pre-processor we defined above, and the pre-processor will process different features based on which type group it is in with using the corresponding transformer\n",
    "                     ('normalizer', Normalizer()),\n",
    "                       ('classifier', SVC( # the second step is choosing the search space (which model we will use)\n",
    "                          random_state=1, max_iter=10000))])\n",
    "param_svc = { # the range of hyper-parameters we want to search\n",
    "   'preprocessor__num__iterative_imputer__max_iter': range(10,20),\n",
    "    # 'preprocessor__num__imputer__strategy': ['mean','median','most_frequent'], # search the optimal hyper-parameter setting for the pre-processor\n",
    "    'classifier__kernel':['linear','poly', 'rbf', 'sigmoid','precomputed'], \n",
    "    'classifier__gamma': ['auto','scale'],\n",
    "    'classifier__degree': range(1,10), # set degree for poly kernel use only, the default value is 3\n",
    "    'classifier__coef0': range(0,10), #  only used in ‘poly’ and ‘sigmoid’, default value is 0,\n",
    "    'classifier__tol': [1e-4,1e-3], # default value is 1e-3\n",
    "    'classifier__decision_function_shape': ['ovr','ovo'], # default value is 'ovr', and 'ovo' cannot be used when is 'break_ties=True'\n",
    "#    'classifier__break_ties': [True, False], # This parameter can only be 'True' when decision_function_shape='ovr', so the default value is 'False'. However, tuning this parameter is redundant as 'False' turns out to be the best parameter setting for 'break_ties' \n",
    "    'classifier__cache_size': [200,300], # Specify the size of the kernel cache size (MB), the default value is 200\n",
    "    'normalizer__norm': ['l1','l2','max'],\n",
    "}\n",
    "random_search_svc = RandomizedSearchCV( # pass the model pipeline and the ranges of (hyper-)parameters we want to search as arguments to RandomizedSearchCV()\n",
    "    regr, param_svc, cv=5, verbose=3, n_jobs=-1,  # cv=5 means we have 5 folds for the CV, n_jobs = -1 means using all CPU processors\n",
    "    n_iter=35,random_state=1,  # 'n_iter'=35 means that 35 parameter settings are randomly sampled, and so we have 35 models that will go through the 5-fold cross-validation\n",
    "    scoring='f1_micro') \n",
    "\n",
    "random_search_svc.fit(X_train, y_train)\n",
    "print('best score {}'.format(random_search_svc.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocessor__num__iterative_imputer__max_iter': 17,\n",
       " 'normalizer__norm': 'l2',\n",
       " 'classifier__tol': 0.001,\n",
       " 'classifier__kernel': 'rbf',\n",
       " 'classifier__gamma': 'scale',\n",
       " 'classifier__degree': 2,\n",
       " 'classifier__decision_function_shape': 'ovr',\n",
       " 'classifier__coef0': 6,\n",
       " 'classifier__cache_size': 300,\n",
       " 'classifier__break_ties': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([22.85916519, 19.7175808 , 23.04716897, 24.0023489 , 11.26818204]),\n",
       " 'score_time': array([3.07712007, 2.68952012, 3.0738728 , 2.7542901 , 1.40864897]),\n",
       " 'test_accuracy': array([0.70137525, 0.72870249, 0.72411533, 0.71625164, 0.71559633])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring = ['accuracy'] \n",
    "scores = cross_validate(random_search_svc.best_estimator_, X_train, y_train, scoring=scoring, cv=5, n_jobs=-1) \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is a patch copied from https://github.com/scikit-optimize/scikit-optimize/issues/978\n",
    "# because in the newest skopt version, the parameter 'iid' is removed from BayesSearchCV(), and if we still want to run the code properly, this patch needs to be run first\n",
    "def bayes_search_CV_init(self, estimator, search_spaces, optimizer_kwargs=None,\n",
    "                         n_iter=50, scoring=None, fit_params=None, n_jobs=1,\n",
    "                         n_points=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                         pre_dispatch='2*n_jobs', random_state=None,\n",
    "                         error_score='raise', return_train_score=False):\n",
    "\n",
    "        self.search_spaces = search_spaces\n",
    "        self.n_iter = n_iter\n",
    "        self.n_points = n_points\n",
    "        self.random_state = random_state\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self._check_search_space(self.search_spaces)\n",
    "        self.fit_params = fit_params\n",
    "\n",
    "        super(BayesSearchCV, self).__init__(\n",
    "             estimator=estimator, scoring=scoring,\n",
    "             n_jobs=n_jobs, refit=refit, cv=cv, verbose=verbose,\n",
    "             pre_dispatch=pre_dispatch, error_score=error_score,\n",
    "             return_train_score=return_train_score)\n",
    "        \n",
    "BayesSearchCV.__init__ = bayes_search_CV_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Note: Answers to the word questions in this assignment are documented in 'A1 original_script' notebook\n",
    "### Below is a Cell of Code trying to implement the LGBMClassifier, but I failed to make it running on my Macbook as I need to download and install multiple softwares to make it run and I do not have that mcuh time \n",
    "### (So you can ignore the below codes though I believe it can be run if you have the 'lightgbm' package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/opt/anaconda3/envs/test372/lib/python3.7/site-packages/lightgbm/lib_lightgbm.so, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: /opt/anaconda3/envs/test372/lib/python3.7/site-packages/lightgbm/lib_lightgbm.so\n  Reason: image not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-732-bd4d2255a4a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define another pipeline of building/training a different classifier/model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m regr = Pipeline(steps=[('preprocessor', preprocessor), # the first step is do the pre-processing with using the pre-processor we defined above, and the pre-processor will process different features based on which type group it is in with using the corresponding transformer\n\u001b[1;32m      4\u001b[0m                      \u001b[0;34m(\u001b[0m\u001b[0;34m'normalizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        ('classifier', LGBMClassifier( # the second step is choosing the search space (which model we will use)\n",
      "\u001b[0;32m/opt/anaconda3/envs/test372/lib/python3.7/site-packages/lightgbm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from .callback import (early_stopping, print_evaluation, record_evaluation,\n\u001b[1;32m     10\u001b[0m                        reset_parameter)\n",
      "\u001b[0;32m/opt/anaconda3/envs/test372/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/test372/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCFUNCTYPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/test372/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36mLoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dlltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0mcdll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLibraryLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/test372/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/opt/anaconda3/envs/test372/lib/python3.7/site-packages/lightgbm/lib_lightgbm.so, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: /opt/anaconda3/envs/test372/lib/python3.7/site-packages/lightgbm/lib_lightgbm.so\n  Reason: image not found"
     ]
    }
   ],
   "source": [
    "# define another pipeline of building/training a different classifier/model\n",
    "from lightgbm import LGBMClassifier\n",
    "regr = Pipeline(steps=[('preprocessor', preprocessor), # the first step is do the pre-processing with using the pre-processor we defined above, and the pre-processor will process different features based on which type group it is in with using the corresponding transformer\n",
    "                     ('normalizer', Normalizer()),\n",
    "                       ('classifier', LGBMClassifier( # the second step is choosing the search space (which model we will use)\n",
    "                        objective='multiclass'))]) # the objective is multiclass for LGBMClassifier as we have three types of label values\n",
    "distribution = { # the range of hyper-parameters we want to search\n",
    "   'preprocessor__num__iterative_imputer__max_iter': range(5,20),# use [5,10,15], in GridSearchCV \n",
    " #   'preprocessor__num__imputer__strategy': ['mean'],#'median','most_frequent'], # search the optimal hyper-parameter setting for the pre-processor\n",
    "    'classifier__boosting_type': ['gbdt', 'dart','goss','rf'],#'rank:pairwise','rank:map' ], \n",
    "    #'classifier__eval_metric': ['merror','map','mlogloss','aucpr'], # merror = Multiclass classification error rate  \n",
    "    'classifier__max_depth': range(5,20), # use [6, 10], in GridSearchCV\n",
    "    'classifier__n_estimator': range(100,250), # use [100,200], in GridSearchCV\n",
    "    'classifier__colsample_bytree': range(0,1), # the range of colsample_bytree is (0,1] \n",
    "    'classifier__colsample_bylevel': range(0,1),\n",
    "    'normalizer__norm': ['max'] # 'l1','l2',\n",
    "}\n",
    "random_search = RandomizedSearchCV( # putting the model pipeline, the range of hyper-parameters we want to search,\n",
    "    regr, distribution, cv=5, verbose=1, n_jobs=-1,  # cv=5 means we have 5 folds for the CV, n_jobs = 2 means number of CPU we want to use\n",
    "    n_iter=60,random_state=0,\n",
    "    scoring='f1')\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print('best score {}'.format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy'] # you can include as many scores as you want\n",
    "scores = cross_validate(random_search.best_estimator_, X_train, y_train, scoring=scoring, cv=5, n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "A1-example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
