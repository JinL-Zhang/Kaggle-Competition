{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gOAPigPV2DPz"
   },
   "outputs": [],
   "source": [
    "\n",
    "!wget -q https://github.com/CISC-372/Notebook/releases/download/a4/test.csv\n",
    "!wget -q https://github.com/CISC-372/Notebook/releases/download/a4/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "HObdr_fR2J7a",
    "outputId": "7e8ef8e7-a1a6-4912-a4d5-82b677e2a511"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>2596</td>\n",
       "      <td>1</td>\n",
       "      <td>Five Stars_GOOD</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <td>4643</td>\n",
       "      <td>1</td>\n",
       "      <td>Love it_Love it</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>8791</td>\n",
       "      <td>1</td>\n",
       "      <td>Five Stars_Good</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3902</th>\n",
       "      <td>6098</td>\n",
       "      <td>1</td>\n",
       "      <td>Five Stars_love!</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>4609</td>\n",
       "      <td>1</td>\n",
       "      <td>love these_so cute!</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>518</td>\n",
       "      <td>1</td>\n",
       "      <td>So far, it's awesome_Ok, so I'll say up front ...</td>\n",
       "      <td>5765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>It Works (Read Tips For Potential Effectivenes...</td>\n",
       "      <td>6740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5046</th>\n",
       "      <td>7257</td>\n",
       "      <td>1</td>\n",
       "      <td>An exquisitely effective product with an astou...</td>\n",
       "      <td>8082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4859</th>\n",
       "      <td>7555</td>\n",
       "      <td>1</td>\n",
       "      <td>Gorgeous professional looking manicure at home...</td>\n",
       "      <td>8134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>4823</td>\n",
       "      <td>1</td>\n",
       "      <td>WAITED TO WRITE THIS REVIEW UNTIL AFTER READIN...</td>\n",
       "      <td>12773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6223 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  rating                                             review  length\n",
       "6037  2596       1                                    Five Stars_GOOD      15\n",
       "5353  4643       1                                    Love it_Love it      15\n",
       "2545  8791       1                                    Five Stars_Good      15\n",
       "3902  6098       1                                   Five Stars_love!      16\n",
       "2850  4609       1                                love these_so cute!      19\n",
       "...    ...     ...                                                ...     ...\n",
       "5651   518       1  So far, it's awesome_Ok, so I'll say up front ...    5765\n",
       "1615   124       1  It Works (Read Tips For Potential Effectivenes...    6740\n",
       "5046  7257       1  An exquisitely effective product with an astou...    8082\n",
       "4859  7555       1  Gorgeous professional looking manicure at home...    8134\n",
       "2758  4823       1  WAITED TO WRITE THIS REVIEW UNTIL AFTER READIN...   12773\n",
       "\n",
       "[6223 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comment your understanding of each function \n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "xy_train_df = pd.read_csv('train.csv')\n",
    "x_test_df  = pd.read_csv('test.csv', index_col='id')\n",
    "\n",
    "\n",
    "xy_train_df['length'] = xy_train_df.apply(lambda x: len(x.review), axis=1)\n",
    "xy_train_df = xy_train_df.sort_values('length')\n",
    "xy_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFo8SePW2KBd",
    "outputId": "d36426f5-a0c1-4598-a618-9df38466e913"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "vocab_size = 10000 # change vocabulary size to 30000, then to 40000\n",
    "max_len = 288     # change max_len to 512, 256, 320\n",
    "\n",
    "xy_train, xy_validation = train_test_split(\n",
    "    xy_train_df, test_size=0.2)\n",
    "\n",
    "# build vocabulary from the training set\n",
    "tokenizer = Tokenizer(num_words=vocab_size,) \n",
    "tokenizer.fit_on_texts(xy_train.review) \n",
    "\n",
    "\n",
    "def _preprocess(texts):\n",
    "  # Note: the proprecessing techniques are added in 2nd tuning of GRU model (and is removed when training bidirectional GRU)\n",
    "  texts_ap=[] # initialize the list that will contains the samples after implementing the preprocessing techniques\n",
    "  # Added preprocessing for each sample in the argument 'texts' through a for loop\n",
    "  ps = PorterStemmer()    # intialize the stemmer to do the stemming\n",
    "  stop_words = set(stopwords.words('english'))  # get the set of English stopwords from the nltk package \n",
    "\n",
    "#  for i in range(0, texts.size):\n",
    "#    review = re.sub('[^a-zA-Z]', ' ', texts.iloc[i])  # replace all the characters of the current text sample that are not in the alphabet with a empty space ' '\n",
    "   # review = review.lower() # added case-normalization in 8th tuning of GRU model: Convert all English characters in the current sample into lower-case letters\n",
    "                             # removed in 9th tuning of GRU model as it deteriorates the model performace \n",
    "#    review = review.split() # split the current text sample into list of words, so the preprocessing techniques can be implemented   \n",
    "#    review = [w for w in review if not w in stop_words] # Preprocessing technique: 'stopwords removal': remove all stopwords from the current text sample\n",
    "   # review = [ps.stem(w) for w in review]   # Preprocessing technique: 'stemming': perform stemming for each word/token in the current text sample\n",
    "                             # removed in 10th tuning of GRU model as it deteriorates the model performace \n",
    "#    review = ' '.join(review)   # join the individual tokens/words back into a sentence (with space between each word/token) \n",
    "#    texts_ap.append(review) # append the processed text sample into the list 'texts_ap'\n",
    "  \n",
    "  return pad_sequences(\n",
    "      tokenizer.texts_to_sequences(texts), #texts_ap\n",
    "      maxlen=max_len, \n",
    "      padding='post'\n",
    "  )\n",
    "\n",
    "\n",
    "x_train = _preprocess(xy_train.review)\n",
    "y_train = xy_train.rating\n",
    "\n",
    "x_valid = _preprocess(xy_validation.review)\n",
    "y_valid = xy_validation.rating\n",
    "\n",
    "x_test = _preprocess(x_test_df.review)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)\n",
    "tokenizer.get_config() # returns the tokenizer configuration as Python Dictionary (including 'word_counts', which counting the frequency of occurrence of each word in the training set)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1kzSY6o2KGR",
    "outputId": "f0ba2533-1816-4248-8e54-c193562da170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 288)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 288, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLam (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,341,889\n",
      "Trainable params: 1,341,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# comment your understanding of each line and \n",
    "# the output shape of each line below. for each dimensionality, explains its \n",
    "# meaning. (e.g. None is the batch size)\n",
    "\n",
    "\n",
    "# since Keras 2.0 metrics f1, precision, and recall have been removed, we have to code the f1 metric function for the evaluation of the model performance \n",
    "# Below is a custom f1 metric function retrieved from https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras:\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "x = keras.Input((max_len)) # input layer\n",
    "\n",
    "embeded = keras.layers.Embedding(vocab_size, 100)(x) # embedding layer\n",
    "\n",
    "averaged = tf.reduce_mean(embeded, axis=1) # average layer to reduce the rank-3 tensor into a 2d matrix\n",
    "\n",
    "# multiple Dense layer, Fully-Connected NN\n",
    "averaged5 = Dense(128,activation=None)(averaged)\n",
    "\n",
    "averaged6 = Dense(256,activation='relu')(averaged5)\n",
    "\n",
    "averaged2 = Dense(512,activation='relu')(averaged6)\n",
    "\n",
    "averaged4 = Dense(256,activation='relu')(averaged2)\n",
    "\n",
    "averaged3 = Dense(128,activation='relu')(averaged4)\n",
    "\n",
    "pred = keras.layers.Dense(1, activation=tf.nn.sigmoid)(averaged3) # output layer\n",
    "\n",
    "model = keras.Model(x, pred)\n",
    "\n",
    "# such design of the model gives 93.2% on the Kaggle public leaderboard\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(clipnorm=None),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy',f1])\n",
    "\n",
    "model.summary() # print out a summary table of the model structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjisOi-t2KJt",
    "outputId": "7b85b8c4-8704-48cd-a997-04f1ea33d2c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10/10 [==============================] - 5s 97ms/step - loss: 0.6226 - accuracy: 0.8697 - f1: 0.9303 - val_loss: 0.4053 - val_accuracy: 0.8859 - val_f1: 0.9415\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.4181 - accuracy: 0.8790 - f1: 0.9355 - val_loss: 0.3913 - val_accuracy: 0.8859 - val_f1: 0.9415\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.4016 - accuracy: 0.8699 - f1: 0.9304 - val_loss: 0.3616 - val_accuracy: 0.8859 - val_f1: 0.9415\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.3764 - accuracy: 0.8761 - f1: 0.9340 - val_loss: 0.3584 - val_accuracy: 0.8859 - val_f1: 0.9415\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.3745 - accuracy: 0.8738 - f1: 0.9327 - val_loss: 0.3493 - val_accuracy: 0.8859 - val_f1: 0.9415\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.3741 - accuracy: 0.8711 - f1: 0.9311 - val_loss: 0.3435 - val_accuracy: 0.8859 - val_f1: 0.9415\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.3512 - accuracy: 0.8793 - f1: 0.9356 - val_loss: 0.3343 - val_accuracy: 0.8859 - val_f1: 0.9415\n",
      "Epoch 8/30\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.3391 - accuracy: 0.8741 - f1: 0.9328 - val_loss: 0.2922 - val_accuracy: 0.8859 - val_f1: 0.9415\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.2708 - accuracy: 0.8744 - f1: 0.9328 - val_loss: 0.2124 - val_accuracy: 0.8859 - val_f1: 0.9415\n",
      "Epoch 10/30\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.1721 - accuracy: 0.8885 - f1: 0.9401 - val_loss: 0.1890 - val_accuracy: 0.9317 - val_f1: 0.9628\n",
      "Epoch 11/30\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.1283 - accuracy: 0.9630 - f1: 0.9789 - val_loss: 0.1942 - val_accuracy: 0.9181 - val_f1: 0.9543\n",
      "Epoch 12/30\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.1013 - accuracy: 0.9671 - f1: 0.9811 - val_loss: 0.1985 - val_accuracy: 0.9365 - val_f1: 0.9651\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0600 - accuracy: 0.9804 - f1: 0.9888 - val_loss: 0.2231 - val_accuracy: 0.9430 - val_f1: 0.9675\n",
      "Epoch 14/30\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0408 - accuracy: 0.9874 - f1: 0.9928 - val_loss: 0.2416 - val_accuracy: 0.9293 - val_f1: 0.9609\n",
      "Epoch 15/30\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0268 - accuracy: 0.9935 - f1: 0.9963 - val_loss: 0.2480 - val_accuracy: 0.9438 - val_f1: 0.9679\n",
      "Epoch 16/30\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.0174 - accuracy: 0.9949 - f1: 0.9970 - val_loss: 0.2900 - val_accuracy: 0.9430 - val_f1: 0.9675\n",
      "Epoch 17/30\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.0102 - accuracy: 0.9975 - f1: 0.9986 - val_loss: 0.3240 - val_accuracy: 0.9406 - val_f1: 0.9661\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.0086 - accuracy: 0.9986 - f1: 0.9992 - val_loss: 0.3587 - val_accuracy: 0.9325 - val_f1: 0.9626\n",
      "Epoch 19/30\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0077 - accuracy: 0.9982 - f1: 0.9990 - val_loss: 0.3809 - val_accuracy: 0.9309 - val_f1: 0.9624\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0043 - accuracy: 0.9992 - f1: 0.9996 - val_loss: 0.4039 - val_accuracy: 0.9333 - val_f1: 0.9635\n",
      "Epoch 21/30\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0060 - accuracy: 0.9989 - f1: 0.9994 - val_loss: 0.4019 - val_accuracy: 0.9349 - val_f1: 0.9635\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.0043 - accuracy: 0.9993 - f1: 0.9996 - val_loss: 0.4388 - val_accuracy: 0.9309 - val_f1: 0.9620\n",
      "Epoch 23/30\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0056 - accuracy: 0.9987 - f1: 0.9992 - val_loss: 0.5268 - val_accuracy: 0.9253 - val_f1: 0.9598\n",
      "Epoch 24/30\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.0043 - accuracy: 0.9990 - f1: 0.9994 - val_loss: 0.4835 - val_accuracy: 0.9325 - val_f1: 0.9631\n",
      "Epoch 25/30\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.0012 - accuracy: 0.9997 - f1: 0.9998 - val_loss: 0.4778 - val_accuracy: 0.9285 - val_f1: 0.9603\n",
      "Epoch 26/30\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 2.3693e-04 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 0.5252 - val_accuracy: 0.9285 - val_f1: 0.9613\n",
      "Epoch 27/30\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 2.1630e-04 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.9325 - val_f1: 0.9634\n",
      "Epoch 28/30\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 1.3512e-04 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 0.5160 - val_accuracy: 0.9325 - val_f1: 0.9634\n",
      "Epoch 29/30\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 1.0978e-04 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 0.5330 - val_accuracy: 0.9325 - val_f1: 0.9635\n",
      "Epoch 30/30\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 7.8524e-05 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 0.5367 - val_accuracy: 0.9333 - val_f1: 0.9638\n"
     ]
    }
   ],
   "source": [
    "# use Callback() method to save the the model weights that yield the highest validation f1\n",
    "checkpoint_filepath = '/checkpoint' # set the path to save the check_point model parameters\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( # define the Callback() function\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_f1', \n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[model_checkpoint_callback],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlkb0lX33YQc",
    "outputId": "ace0a448-d0ff-4c03-9757-7b6ac47c663a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 3ms/step - loss: 0.5367 - accuracy: 0.9333 - f1: 0.9622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5367026925086975, 0.9333333373069763, 0.9622277617454529]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IkK1Dtz3iaQ",
    "outputId": "a1b6633e-2fa2-47a7-b5ff-c87a879d2f9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "def predict_class(_dataset):\n",
    "  classes = model.predict(_dataset) > 0.5\n",
    "  return np.squeeze(classes * 1) \n",
    "\n",
    "y_predict = predict_class(x_valid)\n",
    "\n",
    "from sklearn.metrics import  f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(f1_score(y_valid, y_predict, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "hNXAkglE3nL2"
   },
   "outputs": [],
   "source": [
    "# submission\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.index,\n",
    "     'rating': predict_class(x_test)}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "YHDVHjCP4COL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lc8WejP4QNWj"
   },
   "source": [
    "# Train a 2-layer GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OP8v6OTNQRHg",
    "outputId": "afffe8ee-488e-4a4b-84ab-e7430d5f7056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 288, 400)          4000000   \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 288, 300)          631800    \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 288, 200)          301200    \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 200)               241200    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 5,174,401\n",
      "Trainable params: 5,174,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import GRU, Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Creates the GRU model\n",
    "gru = Sequential()\n",
    "gru.add(Embedding(input_dim = vocab_size, output_dim = 400, input_length=max_len)) # The first layer of the GRU model will always be the embeddinhg layer\n",
    "gru.add(GRU(units=300,return_sequences=True)) # 'return_sequences=True' is required if we want to build multi-layer RNN, because we need the intermediate GRU layers to output the accumulated memory vectors generated from EVERY time-step of the samples,\n",
    "                                              # and so the input to the next GRU layer is a rank-3 tensor of the required shape: [batch_size, number_of_time_step, size_of_the_accumulated_memory_vector or hidden_dim]\n",
    "gru.add(GRU(units=200,return_sequences=True)) # added GRU layer in 6th tuning of GRU model\n",
    "gru.add(GRU(units=200,))\n",
    "gru.add(Dense(units=1, activation='sigmoid',trainable=True)) # the output layer of the GRU model\n",
    "\n",
    "\n",
    "# Note: I define the f1 metric function again in this cell of code in case I (or the TA) jump to run the GRU model code directly\n",
    "# since Keras 2.0 metrics f1, precision, and recall have been removed, we have to code the f1 metric function for the evaluation of the model performance \n",
    "# Below is a custom f1 metric function retrieved from https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras:\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "gru.compile(\n",
    "    optimizer= Adam(clipnorm=None),\n",
    "    #tf.keras.optimizers.Adadelta(clipnorm=None), #Adam(clipnorm=None),\n",
    "    #tf.keras.optimizers.SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),\n",
    "    # start with 'clipnorm=None', try change optimizer to 'tf.keras.optimizers.Adadelta(clipnorm=None)'\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', f1])\n",
    "gru.summary() # print out a summary table of the model structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sbGcpYfR5jC",
    "outputId": "250a5e0d-1de4-4685-8908-a80a273d8318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 39s 1s/step - loss: 0.5733 - accuracy: 0.8674 - f1: 0.9288 - val_loss: 0.3736 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3946 - accuracy: 0.8762 - f1: 0.9339 - val_loss: 0.3742 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3928 - accuracy: 0.8700 - f1: 0.9305 - val_loss: 0.3634 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3787 - accuracy: 0.8766 - f1: 0.9341 - val_loss: 0.3657 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3867 - accuracy: 0.8728 - f1: 0.9321 - val_loss: 0.3566 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3835 - accuracy: 0.8720 - f1: 0.9315 - val_loss: 0.3558 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3897 - accuracy: 0.8687 - f1: 0.9296 - val_loss: 0.3587 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3763 - accuracy: 0.8749 - f1: 0.9332 - val_loss: 0.3548 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3796 - accuracy: 0.8724 - f1: 0.9318 - val_loss: 0.3558 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3676 - accuracy: 0.8780 - f1: 0.9350 - val_loss: 0.3548 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3683 - accuracy: 0.8777 - f1: 0.9348 - val_loss: 0.3547 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3814 - accuracy: 0.8709 - f1: 0.9309 - val_loss: 0.3540 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3681 - accuracy: 0.8774 - f1: 0.9345 - val_loss: 0.3545 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3702 - accuracy: 0.8765 - f1: 0.9341 - val_loss: 0.3546 - val_accuracy: 0.8867 - val_f1: 0.9439\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3829 - accuracy: 0.8694 - f1: 0.9300 - val_loss: 0.3535 - val_accuracy: 0.8867 - val_f1: 0.9439\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3635 - accuracy: 0.8803 - f1: 0.9360 - val_loss: 0.3576 - val_accuracy: 0.8867 - val_f1: 0.9439\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3653 - accuracy: 0.8796 - f1: 0.9356 - val_loss: 0.3540 - val_accuracy: 0.8859 - val_f1: 0.9427\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3628 - accuracy: 0.8807 - f1: 0.9362 - val_loss: 0.3603 - val_accuracy: 0.8859 - val_f1: 0.9427\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3709 - accuracy: 0.8767 - f1: 0.9339 - val_loss: 0.3563 - val_accuracy: 0.8859 - val_f1: 0.9427\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.3695 - accuracy: 0.8780 - f1: 0.9348 - val_loss: 0.3568 - val_accuracy: 0.8851 - val_f1: 0.9424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3337d79590>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Callback() method to save the Keras model or the model weights that yield the highest validation f1\n",
    "checkpoint_filepath = '/checkpoint' # set the path to save the check_point model parameters\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( # define the Callback() function\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_f1', \n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = gru.fit(x_train,\n",
    "                  y_train,\n",
    "                  epochs=20, # change to 20\n",
    "                  batch_size=1024,\n",
    "                  validation_data=(x_valid, y_valid),\n",
    "                  callbacks=[model_checkpoint_callback], # implement Callback() in conjunction with '.fit()' to save a model or weights (in a checkpoint file) that yield the highest validation f1 score\n",
    "                  verbose=1)\n",
    "\n",
    "# put/load the weights into the GRU model\n",
    "gru.load_weights(checkpoint_filepath)\n",
    "\n",
    "# Adjustment log:\n",
    "# Initial design of the GRU model: Vocabulary size is 10000, 'output_dim' of the embedding layer is 200, 'maxlen' of the 'pad_sequences()' is 256 (so the 'input_length' of the embedding layer is 256), \n",
    "# first GRU layer has 'units=200' (so the accumulated memory/knowledge vector has size 200), the second GRU layer has 'units=100', output layer of the GRU model is a Dense layer with only one hidden unit (units=1,activation='sigmoid')\n",
    "# the optimizer chosen is 'Adam' with 'clipnorm=None', batch size chosen is 128\n",
    "# Initial result: validation f1 is stabilized around 93% \n",
    "\n",
    "# 1st tuning: Change the vocabulary size to 30000, change the max_len of each text sample to 512, change 'output_dim' of the embedding layer to 300\n",
    "# result: validation f1 is still stabilized around 93% (no improvement)\n",
    "\n",
    "# 2nd tuning: Added preprocessing techniques (stopword&punctuation removal + stemming)\n",
    "# result: validation f1 is stabilized around 93.5% (slight improvement on the model performance), which means that the added preprocessing techniques can improve the model performance slightly\n",
    "\n",
    "# 3rd tuning: change the 'max_len' of each text sample to 256 to see if shorter the maximal length of each sample (less padding) can affect the model performance or not\n",
    "# result: validation f1 is stabilized around 94% (slight improvement on the model performance), which may indicates that increasing the 'max_len' may deteriorate the model performance \n",
    "# result on Leaderboard: we get a score of 87.3% in the Kaggle public leaderboard\n",
    "\n",
    "# 4th tuning: Change the units of the first GRU layer to 300, Change the units of the second GRU layer to 200, to see if the longer the accumulated memory/knowledge vector size is, the better the model performs\n",
    "# result: validation f1 is still stabilized around 94% (no improvement on the model performance), which may indicates that increasing the 'units' of the GRU layers or the accumulated memory/knowledge vector sizes may not improve the model performance\n",
    "\n",
    "# 5th tuning: Change the optimizer to 'AdaDelta()' to see if changing optimizer can improve model performance or not\n",
    "# result: validation f1 is varying within the range of 93.5% to 94% (no improvement on the model performance)\n",
    "\n",
    "# 6th tuning: Added one more GRU layer with 'units=200', change 'epochs' to 20, change batch_size to 1024  \n",
    "# result: validation f1 is still varying within the range of 93.5% to 94% (no improvement on the model performance), which indicates that adding etra GRU layer will not improve the model's performance\n",
    "\n",
    "# 7th tuning: Change the optimizer to 'SGD with Momentum', change 'output_dim' of the embedding layer to 400\n",
    "# result: validation f1 is still stabilized around 94% , which may indicate that changing the optimizer or increasing the 'size of the meaning vector associated with each token in the sample' may not improve the model performance\n",
    "\n",
    "# 8th tuning: change optimizer back to 'Adam', added the case-normalization preprocessing technique (convert all sample English characters into lower-case) to see if model performance will improve or not\n",
    "# result: validation f1 is stabilized around 93% (decrease in the model performance), which indicates that adding the 'case-normalization' preprocessing technique will actually deteriorate the model performance\n",
    "\n",
    "# 9th tuning: Remove the 'case-normalization' preprocessing technique, Change the vocabulary size to 40000 to see if increasing the vocabulary dictionary size will improve the model performance or not\n",
    "# result: validation f1 is still stabilized around 93%, which indicates that increasing the vocabulary dictionary size will not improve the model performance\n",
    "\n",
    "# 10th tuning: Change the vocabulary size to back to 30000, Remove the 'stemming' preprocessing technique to see if this preprocessing technique actually deteriorates the model performance\n",
    "# result: validation f1 is stabilized around 95% (improvement on the model performance), which suggests that the 'stemming' preprocessing technique actually deteriorates the model performance\n",
    "\n",
    "# 11th tuning: Remove the 'stopwords & punctuation removal' preprocessing technique to see if all the preprocessing techniques are actually not needed as they actually deteriorate the model performance\n",
    "# result: validation f1 is stabilized around 93% (decrease in the model performance), which indicates that the 'stopwords & punctuation removal' preprocessing technique could actually improve the model performance, and so we should add it back \n",
    "\n",
    "# Now we can conclude that no matter how we tune the GRU model, the upper limit of this model is around 94% or 95% in the validation f1, which can be considered as adequately satisfactory model performance, and if we want to further improve the model performance, we should try to use a better model architecture such as the bidirectional GRU\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-DIGzWVY9XZ",
    "outputId": "e505800f-ef59-44a3-b7ff-13ba2004bdea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 1s 25ms/step - loss: 0.3546 - accuracy: 0.8867 - f1: 0.9392\n",
      "0.8867469879518072\n"
     ]
    }
   ],
   "source": [
    "gru.evaluate(x_valid, y_valid)\n",
    "def predict_class(_dataset):\n",
    "  classes = gru.predict(_dataset) > 0.5\n",
    "  return np.squeeze(classes * 1) \n",
    "\n",
    "y_predict = predict_class(x_valid)\n",
    "\n",
    "from sklearn.metrics import  f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(f1_score(y_valid, y_predict, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "MmvHzbxicmNw"
   },
   "outputs": [],
   "source": [
    "# submission\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.index,\n",
    "     'rating': predict_class(x_test)}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "bmqe160n9T5J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcOxwyRJwbUb"
   },
   "source": [
    "# Train a LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zMO3viPwesm",
    "outputId": "973d51c0-1723-4b60-b605-095f79b6e13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 288, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 288, 256)          570368    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,767,617\n",
      "Trainable params: 3,767,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU, Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Creates the LSTM model\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(input_dim = vocab_size, output_dim = 300, input_length=max_len)) # The first layer of RNN model will always be the embeddinhg layer\n",
    "lstm.add(LSTM(units=256,return_sequences=True)) # the first LSTM layer, 'return_sequences=True' is required  \n",
    "lstm.add(LSTM(units=128)) # the second/final LSTM layer, 'return_sequences=False' is required \n",
    "lstm.add(Dense(units=1, activation='sigmoid')) # the output layer of the LSTM model\n",
    "\n",
    "lstm.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(clipnorm=None), #tf.keras.optimizers.SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),#tf.keras.optimizers.Adadelta(clipnorm=None),\n",
    "    # start with 'clipnorm=None', try change optimizer to 'tf.keras.optimizers.Adadelta(clipnorm=None)'\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', f1])\n",
    "\n",
    "lstm.summary() # print out a summary table of the model structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3OUGn_HxXJj",
    "outputId": "6236787e-36ca-4b35-fd71-8ff35f23f2d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 [==============================] - 7s 884ms/step - loss: 0.5954 - accuracy: 0.8698 - f1: 0.9304 - val_loss: 0.4090 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 4s 731ms/step - loss: 0.4358 - accuracy: 0.8678 - f1: 0.9292 - val_loss: 0.3843 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 4s 733ms/step - loss: 0.3943 - accuracy: 0.8737 - f1: 0.9326 - val_loss: 0.3571 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 4s 734ms/step - loss: 0.3848 - accuracy: 0.8734 - f1: 0.9324 - val_loss: 0.3557 - val_accuracy: 0.8859 - val_f1: 0.9437\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 4s 737ms/step - loss: 0.3851 - accuracy: 0.8715 - f1: 0.9313 - val_loss: 0.3593 - val_accuracy: 0.8859 - val_f1: 0.9437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3335766410>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Callback() method to save the Keras model or the model weights that yield the highest validation f1\n",
    "checkpoint_filepath = '/checkpoint' # set the path to save the check_point model parameters\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( # define the Callback() function\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_f1', \n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = lstm.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    verbose=1,\n",
    "                    callbacks=[model_checkpoint_callback]\n",
    "                   )\n",
    "\n",
    "# put/load the weights into the LSTM model\n",
    "lstm.load_weights(checkpoint_filepath)\n",
    "\n",
    "# Adjustment log:\n",
    "# Initial design of the LSTM model: Vocabulary size is 10000, 'output_dim' of the embedding layer is 200, 'maxlen' of the 'pad_sequences()' is 256 (so the 'input_length' of the embedding layer is 256), \n",
    "# first LSTM layer has 'units=256' (so the accumulated memory/knowledge vector has size 256), the second LSTM layer has 'units=128', output layer of the LSTM model is a Dense layer with only one hidden unit (units=1,activation='sigmoid')\n",
    "# the optimizer chosen is 'Adam' with 'clipnorm=None', batch size chosen is 128, epoch chosen is 5\n",
    "# Initial result: validation f1 is also stabilized around 93% (similar performance with the GRU model)\n",
    "\n",
    "# 1st tuning: change 'output_dim' of the embedding layer to 300, change batch_size to 1024\n",
    "# result: validation f1 is still stabilized around 93% (no improvement on the model performance)\n",
    "# Since the LSTM model has a similar performance with the GRU model, we change the model architecture to the bidirecional NN architecture now (instead of continuing tuning the LSTM model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RPvJwPHZ2HcJ",
    "outputId": "b7ba3c63-8eaa-43f3-c7c9-f99f38632724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 1s 16ms/step - loss: 0.4090 - accuracy: 0.8859 - f1: 0.9388\n",
      "0.8859437751004016\n"
     ]
    }
   ],
   "source": [
    "lstm.evaluate(x_valid, y_valid)\n",
    "def predict_class(_dataset):\n",
    "  classes = lstm.predict(_dataset) > 0.5\n",
    "  return np.squeeze(classes * 1) \n",
    "\n",
    "y_predict = predict_class(x_valid)\n",
    "\n",
    "from sklearn.metrics import  f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(f1_score(y_valid, y_predict, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "BBPyZGQm1aev"
   },
   "outputs": [],
   "source": [
    "# submission\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.index,\n",
    "     'rating': predict_class(x_test)}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "iu3QeYOm3oHw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOMhPYVP3oXq"
   },
   "source": [
    "# Train a Bidirectional GRU/LSTM Model with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AGQphfrqshnl"
   },
   "outputs": [],
   "source": [
    "# Attention Mechanism Code is directly retrieved from: https://stackoverflow.com/questions/62948332/how-to-add-attention-layer-to-a-bi-lstm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(Attention,self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                               initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                               initializer=\"zeros\")\n",
    "        \n",
    "        super(Attention,self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        \n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eduA422UImo",
    "outputId": "b49df90a-c7ff-441b-cd71-319c9cf6dd55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 288, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 288, 200)          320800    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 288, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 288, 200)          240800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 288, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 288, 200)          240800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 288, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 288, 200)          240800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 288, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 288, 200)          240800    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 288, 200)          240800    \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 200)               488       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 4,525,489\n",
      "Trainable params: 4,525,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU, Embedding, Dense, LSTM, Bidirectional,Dropout, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Creates the Bidirectional NN model\n",
    "bidir = Sequential()\n",
    "bidir.add(Embedding(input_dim = vocab_size, output_dim = 300, input_length=max_len))  # The first layer of RNN model will always be the embeddinhg layer\n",
    "\n",
    "forward_layer = LSTM(units=100,return_sequences=True) # create the forward layer for the bidrectional RNN\n",
    "\n",
    "# The original first bidirectional layer\n",
    "bidir.add(Bidirectional(forward_layer, )) # Note: No need to specify the 'backward_layer' as this API will automatically create the 'backward_layer' by duplicating the (hyper-)parameters of the 'forward_layer' we specified\n",
    "\n",
    "bidir.add(Dropout(0.2)) # dropout layer added to prevent overfitting\n",
    "bidir.add(Bidirectional(LSTM(units=100,return_sequences=True))) # added bidirectional GRU layer at the 7th tuning of the bidirectional NN model (changed from GRU to LSTM in 13th tuning)\n",
    "bidir.add(Dropout(0.2))\n",
    "bidir.add(Bidirectional(LSTM(units=100,return_sequences=True))) # added bidirectional GRU layer at the 11th tuning of the bidirectional NN model (changed from GRU to LSTM in 13th tuning)\n",
    "bidir.add(Dropout(0.2))\n",
    "bidir.add(Bidirectional(LSTM(units=100,return_sequences=True))) # added bidirectional LSTM layer at the 14th tuning of the bidirectional NN model\n",
    "bidir.add(Dropout(0.2))\n",
    "bidir.add(Bidirectional(LSTM(units=100,return_sequences=True))) # added bidirectional LSTM layer at the 14th tuning of the bidirectional NN model\n",
    "# The original second bidirectional layer\n",
    "bidir.add(Bidirectional(LSTM(units=100,return_sequences=True))) # create the second bidirectional layer with a more concise syntax \n",
    "bidir.add(Attention(return_sequences=False)) # added custom Attention layer at 14th tuning\n",
    "#bidir.add(Dense(units=64, activation='relu')) # added dense layer at 12th tuning (removed at 13th tuning)\n",
    "bidir.add(Dense(units=1, activation='sigmoid')) # the output layer of the bidirectional NN model\n",
    " \n",
    "\n",
    "bidir.compile(\n",
    "    optimizer=  tf.keras.optimizers.Adam(clipnorm=None),\n",
    "    #tf.keras.optimizers.Adam(clipnorm=None),#tf.keras.optimizers.Adadelta(clipnorm=None),  #tf.keras.optimizers.SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),\n",
    "    # start with 'clipnorm=None', try change optimizer to 'tf.keras.optimizers.Adadelta(clipnorm=None)'\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', f1])\n",
    "bidir.summary() # print out a summary table of the model structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5D9mKV-iVzI",
    "outputId": "79b67601-a62b-49d4-f2b8-66f90ab73ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "78/78 [==============================] - 36s 298ms/step - loss: 0.4097 - accuracy: 0.8358 - f1: 0.8854 - val_loss: 0.3234 - val_accuracy: 0.8859 - val_f1: 0.9393\n",
      "Epoch 2/15\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 0.2617 - accuracy: 0.8975 - f1: 0.9424 - val_loss: 0.2075 - val_accuracy: 0.9141 - val_f1: 0.9503\n",
      "Epoch 3/15\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.1175 - accuracy: 0.9579 - f1: 0.9757 - val_loss: 0.2130 - val_accuracy: 0.9237 - val_f1: 0.9563\n",
      "Epoch 4/15\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.0840 - accuracy: 0.9741 - f1: 0.9850 - val_loss: 0.2318 - val_accuracy: 0.9341 - val_f1: 0.9642\n",
      "Epoch 5/15\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.0467 - accuracy: 0.9877 - f1: 0.9928 - val_loss: 0.2323 - val_accuracy: 0.9349 - val_f1: 0.9642\n",
      "Epoch 6/15\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.0299 - accuracy: 0.9939 - f1: 0.9964 - val_loss: 0.3230 - val_accuracy: 0.9181 - val_f1: 0.9532\n",
      "Epoch 7/15\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 0.0251 - accuracy: 0.9938 - f1: 0.9964 - val_loss: 0.2637 - val_accuracy: 0.9141 - val_f1: 0.9507\n",
      "Epoch 8/15\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.0286 - accuracy: 0.9927 - f1: 0.9958 - val_loss: 0.2886 - val_accuracy: 0.9293 - val_f1: 0.9609\n",
      "Epoch 9/15\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 0.0094 - accuracy: 0.9987 - f1: 0.9993 - val_loss: 0.3276 - val_accuracy: 0.9165 - val_f1: 0.9534\n",
      "Epoch 10/15\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.0089 - accuracy: 0.9986 - f1: 0.9992 - val_loss: 0.4097 - val_accuracy: 0.9269 - val_f1: 0.9594\n",
      "Epoch 11/15\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 0.0099 - accuracy: 0.9984 - f1: 0.9991 - val_loss: 0.4230 - val_accuracy: 0.9221 - val_f1: 0.9564\n",
      "Epoch 12/15\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.0115 - accuracy: 0.9975 - f1: 0.9986 - val_loss: 0.3639 - val_accuracy: 0.9269 - val_f1: 0.9591\n",
      "Epoch 13/15\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 0.0144 - accuracy: 0.9977 - f1: 0.9987 - val_loss: 0.3650 - val_accuracy: 0.9149 - val_f1: 0.9522\n",
      "Epoch 14/15\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 0.0103 - accuracy: 0.9978 - f1: 0.9987 - val_loss: 0.3232 - val_accuracy: 0.9092 - val_f1: 0.9467\n",
      "Epoch 15/15\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 0.0162 - accuracy: 0.9972 - f1: 0.9984 - val_loss: 0.4117 - val_accuracy: 0.9205 - val_f1: 0.9557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f31f690bd10>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Callback() method to save the Keras model or the model weights that yield the highest validation f1\n",
    "checkpoint_filepath = '/checkpoint' # set the path to save the check_point model parameters\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( # define the Callback() function\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_f1', # we can try switch 'val_f1' with 'val_accuracy' as usually high validation accuracy comes along with high validation f1\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = bidir.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=15,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    verbose=1,\n",
    "                   callbacks=[model_checkpoint_callback],  # implement Callback() in conjunction with '.fit()' to save a model or weights (in a checkpoint file) that yield the highest validation f1 score\n",
    "                    # class_weight={0: 2.0, 1: 1.0,}, assign different weights to different class labels since the dataset is highly imbalanced (5452 positive cases VS 771 negative cases)\n",
    "                   )\n",
    "\n",
    "# put/load the weights into the bidirectional NN model\n",
    "bidir.load_weights(checkpoint_filepath)\n",
    "\n",
    "# Adjustment log:\n",
    "# Initial design of the bidirectional NN model: Vocabulary size is 10000, 'output_dim' of the embedding layer is 200, 'maxlen' of the 'pad_sequences()' is 256 (so the 'input_length' of the embedding layer is 256), \n",
    "# first bidirectional GRU layer has 'units=200' (so the accumulated memory/knowledge vector has size 200), the second bidirectional GRU layer has 'units=200', output layer of the bidirectional NN model is a Dense layer with only one hidden unit (units=1,activation='sigmoid')\n",
    "# the optimizer chosen is 'Adam' with 'clipnorm=None', batch size chosen is 1024, epoch chosen is 5, added preprocessing techniques (stopword & punctuation removal + stemming)\n",
    "# Initial result: validation f1 is varying within the range of 93.7% to 94.8% (better performance than the GRU and the LSTM model)\n",
    "\n",
    "# 1st tuning: Change the vocabulary size to 30000, change 'output_dim' of the embedding layer to 300, change epoch to 10, change batch_size to 512\n",
    "# result: validation f1 is varying within the range of 93.3% to 94.8% (no improvement on the model performance)\n",
    "# result on Leaderboard: we get a score of 89.9% in the Kaggle public leaderboard\n",
    "\n",
    "# 2nd tuning: remove the 'stemming' preprocessing technique to see if this preprocessing technique actually improve the model performance or not\n",
    "# result: validation f1 is still varying within the range of 93.2% to 94.8% (no improvement on the model performance), which may suggest that the 'stemming' preprocessing technique may not affect the model performance at all\n",
    "\n",
    "# 3rd tuning: Change the units of the first bidirectional layer to 300, Change the units of the second bidirectional layer to 300, to see if the longer the accumulated memory/knowledge vector size is, the better the model performs\n",
    "# result: validation f1 is still varying within the range of 93.2% to 94.8% (no improvement on the model performance), which may indicate that increasing the 'units' of the bidirectional layers or the accumulated memory/knowledge vector sizes may not improve the model performance\n",
    "# Highlight of the result: Both of the training accuracy and f1 are around 99.9% when the number of epoch is above 7, while the validation accuracy and f1 are still around 94%, which may imply that the model is overfitting to the training data\n",
    "\n",
    "# 4th tuning: Remove the 'stopwords & punctuation removal' preprocessing technique to see if all the preprocessing techniques are actually not needed as they actually deteriorate the model performance\n",
    "# result: validation f1 is varying within the range of 93.4% to 95.4% (improvement on the model performance), which may imply that all the preprocessing techniques are actually not needed for the Bidirectional GRU model according to the gained validation f1 score\n",
    "# result on Leaderboard: we get a score of 91.7% in the Kaggle public leaderboard\n",
    "\n",
    "# 5th tuning: Add 2 Dropout layers with 'dropout=0.2' to the bidirectional model to prevent overfitting, and see if the model performance can benefit from the 2 Dropout layers\n",
    "# result: validation f1 is still varying within the range of 93.4% to 95.4%, and both of the training accuracy and f1 are around 99.9% when the number of epoch is above 7, which may indicate that the added 2 Dropout layers for the model may not mitigate the overfitting problem\n",
    "\n",
    "# 6th tuning: Change the units of the first bidirectional layer to 200, Change the units of the second bidirectional layer to 200 to see if the overfitting can be mitigated or not\n",
    "# result: validation f1 is varying within the range of 93.4% to 95% (slight decrease in the model performance), and both of the training accuracy and f1 are around 99.5% when the number of epoch is above 7, which may also indicate that reducing the 'units' of the bidirectional layers or the accumulated memory/knowledge vector sizes may not mitigate the overfitting problem adequately \n",
    "\n",
    "# 7th tuning: Change optimizer to 'AdaDelta', Added one more bidirectional GRU layer with 'units=200',\n",
    "# result: validation f1 is varying within the range of 92.8% to 93.2% (decrease in the model performance), which may indicate that the added bidirectional GRU layer might deteriorate the model performance\n",
    "\n",
    "# 8th tuning: Change optimizer to 'Adam', Change the units of the first & second & thrid bidirectional layer to 300, change batch_size to 256\n",
    "# result: validation f1 is varying within the range of 94.3% to 96.0% (slight improvement on the model performance), which means the conclusion we gained in 7th tuning may be incorrect (the additional bidirectional GRU layer improves the model's performance slightly in fact)\n",
    "# Highlight of the result: Both of the training accuracy and f1 are around 99% when the number of epoch is above 7, so lowering the number of epoch may prevent overfitting\n",
    "\n",
    "# 9th tuning: change the 'max_len' of each text sample to 384 to see if the longer maximal length of each sample (less padding) can affect the model performance or not\n",
    "# result: validation f1 is varying within the range of 94.8% to 96.3% (slight improvement on the model performance), which indicates that increasing the 'max_len' slightly may improve the model performance\n",
    "# result on Leaderboard: we get a score of 91.9% in the Kaggle public leaderboard\n",
    "\n",
    "# 10th tuning: change 'output_dim' of the embedding layer to 400, Change the units of the first & second & thrid bidirectional layer to 400, Change the vocabulary size to 40000 to see if increasing the vocabulary dictionary size, the size of the meaning vector associated with each token in the sample, and the accumulated memory/knowledge vector size will improve the model performance or not\n",
    "# result: validation f1 is still varying within the range of 94.8% to 96.3% (no improvement on the model performance), which imply that simply increasing the vocabulary dictionary size, the size of the meaning vector associated with each token in the sample, and the accumulated memory/knowledge vector size will not improve the model performance\n",
    "\n",
    "# 11th tuning: Added one more bidirectional GRU layer with 'units=400'\n",
    "# result: validation f1 is varying within the range of 93.4% to 95.3% (decrease in the model performance), which may indicate that the extra layer may not improve the model performance anymore\n",
    "\n",
    "# 12th tuning: added one dense layer with 64 hidden units before the output layer of the model\n",
    "# result: validation f1 is varying within the range of 93.4% to 95.3% (no improvement on the model performance), which may imply the added dense layer do not improve the model performance either\n",
    "\n",
    "# 13th tuning: Remove the added one dense layer in 12th tuning, Change the vocabulary size to 50000, Change all bidirectional GRU layers into LSTM layers, Change the units of the first & second & thrid & fourth bidirectional layer to 100, change the 'max_len' of each text sample to 288, change 'out__dim' of the embedding layer to 300,\n",
    "# result: validation f1 is stabilized around 95.5% (improvement on the model performance), which may indicate that LSTM outperforms the GRU under the bidirectional RNN design\n",
    "# result on Leaderboard: we get a score of 92.6% in the Kaggle public leaderboard\n",
    "\n",
    "# 14th tuning: add 2 more bidirectional LSTM layers with 2 Droupout layers with 'dropout=0.2', added attention layer from the external source, Change the vocabulary size to 10000 (large vocabulary size tends to lead overfitting),\n",
    "# result: validation f1 is varying within the range of 94.1% to 96.9% (improvement on the model performance), which may imply that the added attention layer can improve the model performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WM-k2nApkYQI",
    "outputId": "cd1df574-eb88-4672-c0d8-eeda650a0cd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 2s 64ms/step - loss: 0.2323 - accuracy: 0.9349 - f1: 0.9628\n",
      "0.9349397590361446\n"
     ]
    }
   ],
   "source": [
    "bidir.evaluate(x_valid, y_valid)\n",
    "def predict_class(_dataset):\n",
    "  classes = bidir.predict(_dataset) > 0.5\n",
    "  return np.squeeze(classes * 1) \n",
    "\n",
    "y_predict = predict_class(x_valid)\n",
    "\n",
    "from sklearn.metrics import  f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(f1_score(y_valid, y_predict, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "i1JvzfPbmBxE"
   },
   "outputs": [],
   "source": [
    "# submission\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.index,\n",
    "     'rating': predict_class(x_test)}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "AmacvLpwPLmw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "A4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
